{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 28 20:45:13 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   31C    P8     9W / 250W |     19MiB / 11176MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8     8W / 250W |      4MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8    N/A /  N/A |      4MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1446      G   /usr/lib/xorg/Xorg                 14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "from deeplab import DeepLabV3Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Building DeepLabv3Plus Network ***\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 16, 25, 256)\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "*** Output_Shape => (None, 512, 800, 3) ***\n"
     ]
    }
   ],
   "source": [
    "deeplabv3plus = DeepLabV3Plus(img_height=512, img_width=800, nclasses=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 800, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 518, 806, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 256, 400, 64) 9472        zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)   (None, 256, 400, 64) 256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 400, 64) 0           bn_conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 402, 64) 0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 200, 64) 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)         (None, 128, 200, 64) 4160        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizati (None, 128, 200, 64) 256         res2a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 128, 200, 64) 0           bn2a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)         (None, 128, 200, 64) 36928       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizati (None, 128, 200, 64) 256         res2a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 200, 64) 0           bn2a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)         (None, 128, 200, 256 16640       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)          (None, 128, 200, 256 16640       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizati (None, 128, 200, 256 1024        res2a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalizatio (None, 128, 200, 256 1024        res2a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 128, 200, 256 0           bn2a_branch2c[0][0]              \n",
      "                                                                 bn2a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 200, 256 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)         (None, 128, 200, 64) 16448       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizati (None, 128, 200, 64) 256         res2b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 128, 200, 64) 0           bn2b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)         (None, 128, 200, 64) 36928       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizati (None, 128, 200, 64) 256         res2b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 200, 64) 0           bn2b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)         (None, 128, 200, 256 16640       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizati (None, 128, 200, 256 1024        res2b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 128, 200, 256 0           bn2b_branch2c[0][0]              \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 200, 256 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)         (None, 128, 200, 64) 16448       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizati (None, 128, 200, 64) 256         res2c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 128, 200, 64) 0           bn2c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)         (None, 128, 200, 64) 36928       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizati (None, 128, 200, 64) 256         res2c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 128, 200, 64) 0           bn2c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)         (None, 128, 200, 256 16640       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizati (None, 128, 200, 256 1024        res2c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 128, 200, 256 0           bn2c_branch2c[0][0]              \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 128, 200, 256 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)         (None, 64, 100, 128) 32896       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizati (None, 64, 100, 128) 512         res3a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 64, 100, 128) 0           bn3a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)         (None, 64, 100, 128) 147584      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizati (None, 64, 100, 128) 512         res3a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 100, 128) 0           bn3a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)         (None, 64, 100, 512) 66048       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)          (None, 64, 100, 512) 131584      activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizati (None, 64, 100, 512) 2048        res3a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalizatio (None, 64, 100, 512) 2048        res3a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 64, 100, 512) 0           bn3a_branch2c[0][0]              \n",
      "                                                                 bn3a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 100, 512) 0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)         (None, 64, 100, 128) 65664       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizati (None, 64, 100, 128) 512         res3b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 100, 128) 0           bn3b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)         (None, 64, 100, 128) 147584      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizati (None, 64, 100, 128) 512         res3b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 100, 128) 0           bn3b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)         (None, 64, 100, 512) 66048       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizati (None, 64, 100, 512) 2048        res3b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 64, 100, 512) 0           bn3b_branch2c[0][0]              \n",
      "                                                                 activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, 100, 512) 0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)         (None, 64, 100, 128) 65664       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizati (None, 64, 100, 128) 512         res3c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 64, 100, 128) 0           bn3c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)         (None, 64, 100, 128) 147584      activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizati (None, 64, 100, 128) 512         res3c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 64, 100, 128) 0           bn3c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)         (None, 64, 100, 512) 66048       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizati (None, 64, 100, 512) 2048        res3c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 64, 100, 512) 0           bn3c_branch2c[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, 100, 512) 0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)         (None, 64, 100, 128) 65664       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizati (None, 64, 100, 128) 512         res3d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, 100, 128) 0           bn3d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)         (None, 64, 100, 128) 147584      activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizati (None, 64, 100, 128) 512         res3d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64, 100, 128) 0           bn3d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)         (None, 64, 100, 512) 66048       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizati (None, 64, 100, 512) 2048        res3d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 64, 100, 512) 0           bn3d_branch2c[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 100, 512) 0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)         (None, 32, 50, 256)  131328      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 32, 50, 256)  0           bn4a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 50, 256)  0           bn4a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)          (None, 32, 50, 1024) 525312      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalizatio (None, 32, 50, 1024) 4096        res4a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 32, 50, 1024) 0           bn4a_branch2c[0][0]              \n",
      "                                                                 bn4a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 50, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)         (None, 32, 50, 256)  262400      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 32, 50, 256)  0           bn4b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 50, 256)  0           bn4b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 32, 50, 1024) 0           bn4b_branch2c[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 50, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)         (None, 32, 50, 256)  262400      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 32, 50, 256)  0           bn4c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 32, 50, 256)  0           bn4c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4c_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 32, 50, 1024) 0           bn4c_branch2c[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 32, 50, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)         (None, 32, 50, 256)  262400      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4d_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 32, 50, 256)  0           bn4d_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4d_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 32, 50, 256)  0           bn4d_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4d_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 32, 50, 1024) 0           bn4d_branch2c[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 32, 50, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)         (None, 32, 50, 256)  262400      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4e_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 32, 50, 256)  0           bn4e_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4e_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 32, 50, 256)  0           bn4e_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4e_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 32, 50, 1024) 0           bn4e_branch2c[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 32, 50, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)         (None, 32, 50, 256)  262400      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizati (None, 32, 50, 256)  1024        res4f_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 32, 50, 256)  0           bn4f_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)         (None, 32, 50, 256)  590080      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizati (None, 32, 50, 256)  1024        res4f_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 32, 50, 256)  0           bn4f_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)         (None, 32, 50, 1024) 263168      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizati (None, 32, 50, 1024) 4096        res4f_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 32, 50, 1024) 0           bn4f_branch2c[0][0]              \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 32, 50, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)         (None, 16, 25, 512)  524800      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizati (None, 16, 25, 512)  2048        res5a_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 16, 25, 512)  0           bn5a_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)         (None, 16, 25, 512)  2359808     activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizati (None, 16, 25, 512)  2048        res5a_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 16, 25, 512)  0           bn5a_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)         (None, 16, 25, 2048) 1050624     activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)          (None, 16, 25, 2048) 2099200     activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizati (None, 16, 25, 2048) 8192        res5a_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalizatio (None, 16, 25, 2048) 8192        res5a_branch1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 16, 25, 2048) 0           bn5a_branch2c[0][0]              \n",
      "                                                                 bn5a_branch1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 16, 25, 2048) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)         (None, 16, 25, 512)  1049088     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizati (None, 16, 25, 512)  2048        res5b_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 25, 512)  0           bn5b_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)         (None, 16, 25, 512)  2359808     activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizati (None, 16, 25, 512)  2048        res5b_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 16, 25, 512)  0           bn5b_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)         (None, 16, 25, 2048) 1050624     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizati (None, 16, 25, 2048) 8192        res5b_branch2c[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 16, 25, 2048) 0           bn5b_branch2c[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 16, 25, 2048) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)         (None, 16, 25, 512)  1049088     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizati (None, 16, 25, 512)  2048        res5c_branch2a[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 16, 25, 512)  0           bn5c_branch2a[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)         (None, 16, 25, 512)  2359808     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizati (None, 16, 25, 512)  2048        res5c_branch2b[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 16, 25, 512)  0           bn5c_branch2b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 1, 512)       0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 1, 1, 512)    0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "image_pooling (Conv2D)          (None, 1, 1, 256)    131072      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "image_pooling_BN (BatchNormaliz (None, 1, 1, 256)    1024        image_pooling[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aspp0 (Conv2D)                  (None, 16, 25, 256)  131072      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 1, 1, 256)    0           image_pooling_BN[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "aspp0_BN (BatchNormalization)   (None, 16, 25, 256)  1024        aspp0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 16, 25, 256)  0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aspp0_activation (Activation)   (None, 16, 25, 256)  0           aspp0_BN[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aspp1 (SeparableConv2D)         (None, 16, 25, 256)  131584      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aspp2 (SeparableConv2D)         (None, 16, 25, 256)  131584      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aspp3 (SeparableConv2D)         (None, 16, 25, 256)  131584      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "aspp_concat (Concatenate)       (None, 16, 25, 1280) 0           lambda_4[0][0]                   \n",
      "                                                                 aspp0_activation[0][0]           \n",
      "                                                                 aspp1[0][0]                      \n",
      "                                                                 aspp2[0][0]                      \n",
      "                                                                 aspp3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concat_projection (Conv2D)      (None, 16, 25, 256)  327680      aspp_concat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concat_projection_BN (BatchNorm (None, 16, 25, 256)  1024        concat_projection[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 16, 25, 256)  0           concat_projection_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "low_level_projection (Conv2D)   (None, 128, 200, 48) 3072        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16, 25, 256)  0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_low_level_projection (BatchN (None, 128, 200, 48) 192         low_level_projection[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 128, 200, 256 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "low_level_activation (Activatio (None, 128, 200, 48) 0           bn_low_level_projection[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_concat (Concatenate)    (None, 128, 200, 304 0           lambda_5[0][0]                   \n",
      "                                                                 low_level_activation[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv2d_1 (Conv2D)       (None, 128, 200, 256 700416      decoder_concat[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn_decoder_1 (BatchNormalizatio (None, 128, 200, 256 1024        decoder_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_decoder_1 (Activatio (None, 128, 200, 256 0           bn_decoder_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_conv2d_2 (Conv2D)       (None, 128, 200, 256 589824      activation_decoder_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "bn_decoder_2 (BatchNormalizatio (None, 128, 200, 256 1024        decoder_conv2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_decoder_2 (Activatio (None, 128, 200, 256 0           bn_decoder_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 512, 800, 256 0           activation_decoder_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Conv2D)           (None, 512, 800, 3)  771         lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 512, 800, 3)  0           output_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 24,812,867\n",
      "Trainable params: 24,761,187\n",
      "Non-trainable params: 51,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deeplabv3plus.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_train = np.asarray(orig_list)\\ny_train = np.asarray(anot_list)\\nY_train = np.array([image.img_to_array(image.load_img(fname,target_size = (576,768))) for fname in y_train])\\n\\nX_train = np.array([image.img_to_array(image.load_img(fname,target_size = (576,768))) for fname in x_train])'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def sorted_nicely( li ):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(li, key = alphanum_key)\n",
    "    \n",
    "orig_list = glob.glob('/home/sami/Downloads/Canola Dataset/labeled/Final 98/Aug_xtrain/split/*.png')\n",
    "orig_list = sorted_nicely(orig_list)\n",
    "anot_list = glob.glob('/home/sami/Downloads/Canola Dataset/labeled/Final 98/Aug_ytrain/ysplit/*.png')\n",
    "anot_list = sorted_nicely(anot_list)\n",
    "orig_train, orig_test, anot_train, anot_test = train_test_split(orig_list,anot_list,test_size = 0.1)\n",
    "\n",
    "x_train = np.asarray(orig_train)\n",
    "y_train = np.asarray(anot_train)\n",
    "x_val   = np.asarray(orig_test)\n",
    "y_val   = np.asarray(anot_test)\n",
    "\n",
    "Y_train = np.array([image.img_to_array(image.load_img(fname)) for fname in y_train])\n",
    "X_train = np.array([image.img_to_array(image.load_img(fname)) for fname in x_train]) #\n",
    "Y_val = np.array([image.img_to_array(image.load_img(fname)) for fname in y_val])\n",
    "X_val = np.array([image.img_to_array(image.load_img(fname)) for fname in x_val])\n",
    "\"\"\"\n",
    "x_train = np.asarray(orig_list)\n",
    "y_train = np.asarray(anot_list)\n",
    "Y_train = np.array([image.img_to_array(image.load_img(fname,target_size = (576,768))) for fname in y_train])\n",
    "\n",
    "X_train = np.array([image.img_to_array(image.load_img(fname,target_size = (576,768))) for fname in x_train])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5976, 512, 800, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def dice_loss(y_true, y_pred):\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "\n",
    "    return 1 - (numerator + 1) / (denominator + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=5e-4)\n",
    "deeplabv3plus.compile(loss=dice_loss, optimizer=opt, metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=20, verbose=1),\n",
    "    ModelCheckpoint('deeplabv3+withresnet50new.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:169: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deeplabv3plus.load_weights('deeplabv3+withresnet50new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 5976 samples, validate on 664 samples\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9034Epoch 00001: val_loss improved from inf to 0.05593, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1170s 196ms/step - loss: 0.0664 - acc: 0.9035 - val_loss: 0.0559 - val_acc: 0.9163\n",
      "Epoch 2/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9354Epoch 00002: val_loss improved from 0.05593 to 0.04884, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0436 - acc: 0.9354 - val_loss: 0.0488 - val_acc: 0.9298\n",
      "Epoch 3/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9380Epoch 00003: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0416 - acc: 0.9381 - val_loss: 0.1176 - val_acc: 0.8234\n",
      "Epoch 4/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9462Epoch 00004: val_loss improved from 0.04884 to 0.04076, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0361 - acc: 0.9462 - val_loss: 0.0408 - val_acc: 0.9394\n",
      "Epoch 5/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9487Epoch 00005: val_loss improved from 0.04076 to 0.03812, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0344 - acc: 0.9487 - val_loss: 0.0381 - val_acc: 0.9432\n",
      "Epoch 6/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9529Epoch 00006: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0315 - acc: 0.9529 - val_loss: 0.3232 - val_acc: 0.5163\n",
      "Epoch 7/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9533Epoch 00007: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0313 - acc: 0.9533 - val_loss: 0.0434 - val_acc: 0.9349\n",
      "Epoch 8/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9573Epoch 00008: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0286 - acc: 0.9573 - val_loss: 0.0542 - val_acc: 0.9210\n",
      "Epoch 9/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9566Epoch 00009: val_loss improved from 0.03812 to 0.03162, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0291 - acc: 0.9566 - val_loss: 0.0316 - val_acc: 0.9530\n",
      "Epoch 10/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9592Epoch 00010: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0273 - acc: 0.9592 - val_loss: 0.0606 - val_acc: 0.9111\n",
      "Epoch 11/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9605Epoch 00011: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0264 - acc: 0.9605 - val_loss: 0.0453 - val_acc: 0.9323\n",
      "Epoch 12/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9614Epoch 00012: val_loss improved from 0.03162 to 0.03141, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0258 - acc: 0.9615 - val_loss: 0.0314 - val_acc: 0.9530\n",
      "Epoch 13/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9617Epoch 00013: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0256 - acc: 0.9617 - val_loss: 0.0578 - val_acc: 0.9154\n",
      "Epoch 14/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9616Epoch 00014: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0257 - acc: 0.9616 - val_loss: 0.0576 - val_acc: 0.9149\n",
      "Epoch 15/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9623Epoch 00015: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 184ms/step - loss: 0.0252 - acc: 0.9623 - val_loss: 0.0775 - val_acc: 0.8836\n",
      "Epoch 16/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9633Epoch 00016: val_loss improved from 0.03141 to 0.02397, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0246 - acc: 0.9632 - val_loss: 0.0240 - val_acc: 0.9642\n",
      "Epoch 17/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9635Epoch 00017: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0244 - acc: 0.9635 - val_loss: 0.0297 - val_acc: 0.9557\n",
      "Epoch 18/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9613Epoch 00018: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0258 - acc: 0.9614 - val_loss: 0.0241 - val_acc: 0.9640\n",
      "Epoch 19/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9646Epoch 00019: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0237 - acc: 0.9646 - val_loss: 0.0259 - val_acc: 0.9612\n",
      "Epoch 20/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9643Epoch 00020: val_loss improved from 0.02397 to 0.02331, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0238 - acc: 0.9643 - val_loss: 0.0233 - val_acc: 0.9653\n",
      "Epoch 21/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9644Epoch 00021: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0238 - acc: 0.9644 - val_loss: 0.0289 - val_acc: 0.9568\n",
      "Epoch 22/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0232 - acc: 0.9653Epoch 00022: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0232 - acc: 0.9653 - val_loss: 0.0241 - val_acc: 0.9640\n",
      "Epoch 23/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0229 - acc: 0.9657Epoch 00023: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0229 - acc: 0.9657 - val_loss: 0.0431 - val_acc: 0.9354\n",
      "Epoch 24/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9639Epoch 00024: val_loss improved from 0.02331 to 0.02319, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0242 - acc: 0.9639 - val_loss: 0.0232 - val_acc: 0.9653\n",
      "Epoch 25/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0224 - acc: 0.9665Epoch 00025: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0224 - acc: 0.9665 - val_loss: 0.0233 - val_acc: 0.9652\n",
      "Epoch 26/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0228 - acc: 0.9659Epoch 00026: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0228 - acc: 0.9659 - val_loss: 0.0254 - val_acc: 0.9620\n",
      "Epoch 27/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9661Epoch 00027: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0227 - acc: 0.9661 - val_loss: 0.0245 - val_acc: 0.9633\n",
      "Epoch 28/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0223 - acc: 0.9666Epoch 00028: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0223 - acc: 0.9666 - val_loss: 0.0265 - val_acc: 0.9604\n",
      "Epoch 29/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0217 - acc: 0.9675Epoch 00029: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0217 - acc: 0.9675 - val_loss: 0.0268 - val_acc: 0.9598\n",
      "Epoch 30/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9647Epoch 00030: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0236 - acc: 0.9647 - val_loss: 0.0245 - val_acc: 0.9634\n",
      "Epoch 31/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9671Epoch 00031: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0220 - acc: 0.9671 - val_loss: 0.0265 - val_acc: 0.9603\n",
      "Epoch 32/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0227 - acc: 0.9660Epoch 00032: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0228 - acc: 0.9660 - val_loss: 0.1100 - val_acc: 0.8351\n",
      "Epoch 33/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9677Epoch 00033: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0216 - acc: 0.9677 - val_loss: 0.0238 - val_acc: 0.9643\n",
      "Epoch 34/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9678Epoch 00034: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0215 - acc: 0.9678 - val_loss: 0.3044 - val_acc: 0.5440\n",
      "Epoch 35/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0216 - acc: 0.9676Epoch 00035: val_loss improved from 0.02319 to 0.02275, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0216 - acc: 0.9676 - val_loss: 0.0227 - val_acc: 0.9660\n",
      "Epoch 36/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9681Epoch 00036: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 184ms/step - loss: 0.0213 - acc: 0.9681 - val_loss: 0.0248 - val_acc: 0.9630\n",
      "Epoch 37/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9682Epoch 00037: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0213 - acc: 0.9682 - val_loss: 0.0250 - val_acc: 0.9626\n",
      "Epoch 38/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9685Epoch 00038: val_loss improved from 0.02275 to 0.02246, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0211 - acc: 0.9685 - val_loss: 0.0225 - val_acc: 0.9664\n",
      "Epoch 39/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9687Epoch 00039: val_loss did not improve\n",
      "5976/5976 [==============================] - 1102s 184ms/step - loss: 0.0209 - acc: 0.9687 - val_loss: 0.0466 - val_acc: 0.9301\n",
      "Epoch 40/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0211 - acc: 0.9684Epoch 00040: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0211 - acc: 0.9684 - val_loss: 0.0258 - val_acc: 0.9614\n",
      "Epoch 41/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0205 - acc: 0.9693Epoch 00041: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 184ms/step - loss: 0.0205 - acc: 0.9693 - val_loss: 0.0235 - val_acc: 0.9649\n",
      "Epoch 42/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0215 - acc: 0.9679Epoch 00042: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0215 - acc: 0.9679 - val_loss: 0.0252 - val_acc: 0.9623\n",
      "Epoch 43/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0206 - acc: 0.9692Epoch 00043: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0205 - acc: 0.9692 - val_loss: 0.0239 - val_acc: 0.9643\n",
      "Epoch 44/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9691Epoch 00044: val_loss improved from 0.02246 to 0.02189, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0207 - acc: 0.9691 - val_loss: 0.0219 - val_acc: 0.9673\n",
      "Epoch 45/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9697Epoch 00045: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0202 - acc: 0.9697 - val_loss: 0.0247 - val_acc: 0.9631\n",
      "Epoch 46/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9697Epoch 00046: val_loss improved from 0.02189 to 0.02161, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0202 - acc: 0.9697 - val_loss: 0.0216 - val_acc: 0.9676\n",
      "Epoch 47/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9690Epoch 00047: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0207 - acc: 0.9690 - val_loss: 0.0241 - val_acc: 0.9639\n",
      "Epoch 48/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9695Epoch 00048: val_loss improved from 0.02161 to 0.02156, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0204 - acc: 0.9695 - val_loss: 0.0216 - val_acc: 0.9677\n",
      "Epoch 49/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0200 - acc: 0.9701Epoch 00049: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0200 - acc: 0.9701 - val_loss: 0.0223 - val_acc: 0.9666\n",
      "Epoch 50/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0202 - acc: 0.9698Epoch 00050: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0202 - acc: 0.9698 - val_loss: 0.0235 - val_acc: 0.9648\n",
      "Epoch 51/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9702Epoch 00051: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0199 - acc: 0.9702 - val_loss: 0.0218 - val_acc: 0.9674\n",
      "Epoch 52/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9694Epoch 00052: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0204 - acc: 0.9694 - val_loss: 0.0223 - val_acc: 0.9666\n",
      "Epoch 53/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0198 - acc: 0.9704Epoch 00053: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0198 - acc: 0.9704 - val_loss: 0.0227 - val_acc: 0.9660\n",
      "Epoch 54/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9706Epoch 00054: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0196 - acc: 0.9706 - val_loss: 0.0245 - val_acc: 0.9633\n",
      "Epoch 55/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0196 - acc: 0.9706Epoch 00055: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0196 - acc: 0.9706 - val_loss: 0.0225 - val_acc: 0.9663\n",
      "Epoch 56/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9705Epoch 00056: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0197 - acc: 0.9705 - val_loss: 0.0217 - val_acc: 0.9674\n",
      "Epoch 57/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0197 - acc: 0.9706Epoch 00057: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0197 - acc: 0.9706 - val_loss: 0.0226 - val_acc: 0.9662\n",
      "Epoch 58/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9709Epoch 00058: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0194 - acc: 0.9710 - val_loss: 0.0216 - val_acc: 0.9676\n",
      "Epoch 59/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0195 - acc: 0.9709Epoch 00059: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0195 - acc: 0.9709 - val_loss: 0.0218 - val_acc: 0.9673\n",
      "Epoch 60/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9711Epoch 00060: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0193 - acc: 0.9711 - val_loss: 0.0234 - val_acc: 0.9649\n",
      "Epoch 61/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9711Epoch 00061: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0193 - acc: 0.9712 - val_loss: 0.0218 - val_acc: 0.9673\n",
      "Epoch 62/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9709Epoch 00062: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0194 - acc: 0.9709 - val_loss: 0.0238 - val_acc: 0.9644\n",
      "Epoch 63/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9711Epoch 00063: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0193 - acc: 0.9711 - val_loss: 0.0224 - val_acc: 0.9665\n",
      "Epoch 64/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9713Epoch 00064: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0192 - acc: 0.9713 - val_loss: 0.0243 - val_acc: 0.9636\n",
      "Epoch 65/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9712Epoch 00065: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0192 - acc: 0.9712 - val_loss: 0.0217 - val_acc: 0.9675\n",
      "Epoch 66/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9709Epoch 00066: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0194 - acc: 0.9709 - val_loss: 0.0227 - val_acc: 0.9660\n",
      "Epoch 67/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9719Epoch 00067: val_loss improved from 0.02156 to 0.02113, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0188 - acc: 0.9718 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 68/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0191 - acc: 0.9715Epoch 00068: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0191 - acc: 0.9715 - val_loss: 0.0247 - val_acc: 0.9629\n",
      "Epoch 69/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9719Epoch 00069: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0187 - acc: 0.9719 - val_loss: 0.0215 - val_acc: 0.9678\n",
      "Epoch 70/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9720Epoch 00070: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0187 - acc: 0.9719 - val_loss: 0.0216 - val_acc: 0.9676\n",
      "Epoch 71/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9720Epoch 00071: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0187 - acc: 0.9720 - val_loss: 0.0274 - val_acc: 0.9590\n",
      "Epoch 72/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9719Epoch 00072: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0188 - acc: 0.9719 - val_loss: 0.0223 - val_acc: 0.9666\n",
      "Epoch 73/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9719Epoch 00073: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0187 - acc: 0.9719 - val_loss: 0.0225 - val_acc: 0.9663\n",
      "Epoch 74/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9718Epoch 00074: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0188 - acc: 0.9718 - val_loss: 0.0245 - val_acc: 0.9633\n",
      "Epoch 75/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0186 - acc: 0.9721Epoch 00075: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0186 - acc: 0.9721 - val_loss: 0.0232 - val_acc: 0.9652\n",
      "Epoch 76/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9720Epoch 00076: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0187 - acc: 0.9720 - val_loss: 0.0213 - val_acc: 0.9681\n",
      "Epoch 77/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9722Epoch 00077: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0185 - acc: 0.9722 - val_loss: 0.0223 - val_acc: 0.9667\n",
      "Epoch 78/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9724Epoch 00078: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0184 - acc: 0.9724 - val_loss: 0.0237 - val_acc: 0.9645\n",
      "Epoch 79/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9725Epoch 00079: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0184 - acc: 0.9725 - val_loss: 0.0225 - val_acc: 0.9663\n",
      "Epoch 80/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9725Epoch 00080: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0184 - acc: 0.9725 - val_loss: 0.0245 - val_acc: 0.9633\n",
      "Epoch 81/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0185 - acc: 0.9723Epoch 00081: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0185 - acc: 0.9723 - val_loss: 0.0218 - val_acc: 0.9674\n",
      "Epoch 82/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9727Epoch 00082: val_loss improved from 0.02113 to 0.02099, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0182 - acc: 0.9727 - val_loss: 0.0210 - val_acc: 0.9686\n",
      "Epoch 83/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9727Epoch 00083: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0183 - acc: 0.9726 - val_loss: 0.0241 - val_acc: 0.9639\n",
      "Epoch 84/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0184 - acc: 0.9724Epoch 00084: val_loss improved from 0.02099 to 0.02096, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0184 - acc: 0.9724 - val_loss: 0.0210 - val_acc: 0.9686\n",
      "Epoch 85/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9732Epoch 00085: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0179 - acc: 0.9732 - val_loss: 0.0212 - val_acc: 0.9682\n",
      "Epoch 86/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0183 - acc: 0.9726Epoch 00086: val_loss improved from 0.02096 to 0.02062, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0183 - acc: 0.9727 - val_loss: 0.0206 - val_acc: 0.9691\n",
      "Epoch 87/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0181 - acc: 0.9728Epoch 00087: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0181 - acc: 0.9728 - val_loss: 0.0216 - val_acc: 0.9676\n",
      "Epoch 88/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9731Epoch 00088: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0180 - acc: 0.9731 - val_loss: 0.0213 - val_acc: 0.9680\n",
      "Epoch 89/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9730Epoch 00089: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0180 - acc: 0.9730 - val_loss: 0.0226 - val_acc: 0.9661\n",
      "Epoch 90/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0182 - acc: 0.9728Epoch 00090: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0182 - acc: 0.9728 - val_loss: 0.0216 - val_acc: 0.9677\n",
      "Epoch 91/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9733Epoch 00091: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0178 - acc: 0.9733 - val_loss: 0.0210 - val_acc: 0.9685\n",
      "Epoch 92/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9733Epoch 00092: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0178 - acc: 0.9733 - val_loss: 0.0214 - val_acc: 0.9679\n",
      "Epoch 93/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0178 - acc: 0.9734Epoch 00093: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0178 - acc: 0.9734 - val_loss: 0.0239 - val_acc: 0.9643\n",
      "Epoch 94/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9731Epoch 00094: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0180 - acc: 0.9731 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 95/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9737Epoch 00095: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0176 - acc: 0.9736 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 96/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9736Epoch 00096: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0177 - acc: 0.9735 - val_loss: 0.0210 - val_acc: 0.9685\n",
      "Epoch 97/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9736Epoch 00097: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0177 - acc: 0.9736 - val_loss: 0.0214 - val_acc: 0.9679\n",
      "Epoch 98/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9736Epoch 00098: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0176 - acc: 0.9736 - val_loss: 0.0245 - val_acc: 0.9633\n",
      "Epoch 99/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9735Epoch 00099: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0177 - acc: 0.9736 - val_loss: 0.0212 - val_acc: 0.9682\n",
      "Epoch 100/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9738Epoch 00100: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0175 - acc: 0.9738 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 101/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9738Epoch 00101: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0175 - acc: 0.9738 - val_loss: 0.0234 - val_acc: 0.9650\n",
      "Epoch 102/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9738Epoch 00102: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0175 - acc: 0.9738 - val_loss: 0.0227 - val_acc: 0.9659\n",
      "Epoch 103/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0174 - acc: 0.9740Epoch 00103: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0174 - acc: 0.9740 - val_loss: 0.0210 - val_acc: 0.9685\n",
      "Epoch 104/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9738Epoch 00104: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0175 - acc: 0.9738 - val_loss: 0.0249 - val_acc: 0.9627\n",
      "Epoch 105/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9732Epoch 00105: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0179 - acc: 0.9732 - val_loss: 0.0220 - val_acc: 0.9670\n",
      "Epoch 106/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9742Epoch 00106: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0172 - acc: 0.9742 - val_loss: 0.0224 - val_acc: 0.9664\n",
      "Epoch 107/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9741Epoch 00107: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0173 - acc: 0.9741 - val_loss: 0.0209 - val_acc: 0.9687\n",
      "Epoch 108/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9741Epoch 00108: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0173 - acc: 0.9741 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 109/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9742Epoch 00109: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0172 - acc: 0.9742 - val_loss: 0.0227 - val_acc: 0.9660\n",
      "Epoch 110/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9742Epoch 00110: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0172 - acc: 0.9742 - val_loss: 0.0206 - val_acc: 0.9691\n",
      "Epoch 111/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9743Epoch 00111: val_loss improved from 0.02062 to 0.02054, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0171 - acc: 0.9743 - val_loss: 0.0205 - val_acc: 0.9692\n",
      "Epoch 112/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9743Epoch 00112: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0171 - acc: 0.9743 - val_loss: 0.0212 - val_acc: 0.9682\n",
      "Epoch 113/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9741Epoch 00113: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0173 - acc: 0.9741 - val_loss: 0.0208 - val_acc: 0.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9745Epoch 00114: val_loss improved from 0.02054 to 0.02025, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0170 - acc: 0.9745 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 115/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9744Epoch 00115: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0171 - acc: 0.9744 - val_loss: 0.0209 - val_acc: 0.9687\n",
      "Epoch 116/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9746Epoch 00116: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0170 - acc: 0.9746 - val_loss: 0.0206 - val_acc: 0.9691\n",
      "Epoch 117/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9746Epoch 00117: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0169 - acc: 0.9746 - val_loss: 0.0248 - val_acc: 0.9628\n",
      "Epoch 118/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9746Epoch 00118: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0170 - acc: 0.9746 - val_loss: 0.0230 - val_acc: 0.9655\n",
      "Epoch 119/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9747Epoch 00119: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0169 - acc: 0.9747 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 120/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9746Epoch 00120: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0169 - acc: 0.9746 - val_loss: 0.0205 - val_acc: 0.9693\n",
      "Epoch 121/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9743Epoch 00121: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0172 - acc: 0.9743 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 122/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9750Epoch 00122: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0167 - acc: 0.9750 - val_loss: 0.0224 - val_acc: 0.9664\n",
      "Epoch 123/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9745Epoch 00123: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0170 - acc: 0.9745 - val_loss: 0.0203 - val_acc: 0.9695\n",
      "Epoch 124/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9748Epoch 00124: val_loss improved from 0.02025 to 0.02003, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0168 - acc: 0.9748 - val_loss: 0.0200 - val_acc: 0.9700\n",
      "Epoch 125/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9748Epoch 00125: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0168 - acc: 0.9749 - val_loss: 0.0203 - val_acc: 0.9695\n",
      "Epoch 126/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9750Epoch 00126: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0167 - acc: 0.9750 - val_loss: 0.0203 - val_acc: 0.9695\n",
      "Epoch 127/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9750Epoch 00127: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0167 - acc: 0.9750 - val_loss: 0.0213 - val_acc: 0.9681\n",
      "Epoch 128/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9751Epoch 00128: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0166 - acc: 0.9751 - val_loss: 0.0204 - val_acc: 0.9695\n",
      "Epoch 129/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9749Epoch 00129: val_loss improved from 0.02003 to 0.02003, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0168 - acc: 0.9749 - val_loss: 0.0200 - val_acc: 0.9700\n",
      "Epoch 130/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9751Epoch 00130: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0166 - acc: 0.9751 - val_loss: 0.0215 - val_acc: 0.9678\n",
      "Epoch 131/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9751Epoch 00131: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0166 - acc: 0.9752 - val_loss: 0.0232 - val_acc: 0.9653\n",
      "Epoch 132/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9752Epoch 00132: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0165 - acc: 0.9752 - val_loss: 0.0205 - val_acc: 0.9692\n",
      "Epoch 133/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9753Epoch 00133: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0165 - acc: 0.9753 - val_loss: 0.0223 - val_acc: 0.9666\n",
      "Epoch 134/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9751Epoch 00134: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0166 - acc: 0.9751 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 135/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9753Epoch 00135: val_loss improved from 0.02003 to 0.01957, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0165 - acc: 0.9753 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 136/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9754Epoch 00136: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0164 - acc: 0.9754 - val_loss: 0.0205 - val_acc: 0.9693\n",
      "Epoch 137/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9753Epoch 00137: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0165 - acc: 0.9753 - val_loss: 0.0206 - val_acc: 0.9691\n",
      "Epoch 138/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0164 - acc: 0.9754Epoch 00138: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0164 - acc: 0.9754 - val_loss: 0.0269 - val_acc: 0.9597\n",
      "Epoch 139/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9753Epoch 00139: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0165 - acc: 0.9753 - val_loss: 0.0202 - val_acc: 0.9697\n",
      "Epoch 140/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9753Epoch 00140: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0165 - acc: 0.9753 - val_loss: 0.0207 - val_acc: 0.9689\n",
      "Epoch 141/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00141: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0163 - acc: 0.9756 - val_loss: 0.0202 - val_acc: 0.9697\n",
      "Epoch 142/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00142: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0163 - acc: 0.9756 - val_loss: 0.0242 - val_acc: 0.9637\n",
      "Epoch 143/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00143: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0163 - acc: 0.9756 - val_loss: 0.0197 - val_acc: 0.9704\n",
      "Epoch 144/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00144: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0163 - acc: 0.9756 - val_loss: 0.0239 - val_acc: 0.9642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00145: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0162 - acc: 0.9756 - val_loss: 0.0218 - val_acc: 0.9674\n",
      "Epoch 146/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9758Epoch 00146: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0162 - acc: 0.9758 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 147/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0162 - acc: 0.9757Epoch 00147: val_loss improved from 0.01957 to 0.01943, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0162 - acc: 0.9757 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 148/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9758Epoch 00148: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0161 - acc: 0.9758 - val_loss: 0.0205 - val_acc: 0.9693\n",
      "Epoch 149/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0168 - acc: 0.9748Epoch 00149: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0168 - acc: 0.9748 - val_loss: 0.0205 - val_acc: 0.9693\n",
      "Epoch 150/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9761Epoch 00150: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0160 - acc: 0.9761 - val_loss: 0.0201 - val_acc: 0.9699\n",
      "Epoch 151/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9759Epoch 00151: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0161 - acc: 0.9759 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 152/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9759Epoch 00152: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0161 - acc: 0.9759 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 153/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9760Epoch 00153: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0160 - acc: 0.9760 - val_loss: 0.0230 - val_acc: 0.9655\n",
      "Epoch 154/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9760Epoch 00154: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0160 - acc: 0.9760 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 155/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9761Epoch 00155: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0160 - acc: 0.9761 - val_loss: 0.0202 - val_acc: 0.9697\n",
      "Epoch 156/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0160 - acc: 0.9760Epoch 00156: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0160 - acc: 0.9760 - val_loss: 0.0210 - val_acc: 0.9685\n",
      "Epoch 157/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9762Epoch 00157: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0159 - acc: 0.9762 - val_loss: 0.0213 - val_acc: 0.9680\n",
      "Epoch 158/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9761Epoch 00158: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0159 - acc: 0.9761 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 159/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9762Epoch 00159: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0159 - acc: 0.9762 - val_loss: 0.0213 - val_acc: 0.9681\n",
      "Epoch 160/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9756Epoch 00160: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0162 - acc: 0.9756 - val_loss: 0.0201 - val_acc: 0.9699\n",
      "Epoch 161/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9764Epoch 00161: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0157 - acc: 0.9764 - val_loss: 0.0210 - val_acc: 0.9685\n",
      "Epoch 162/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9764Epoch 00162: val_loss improved from 0.01943 to 0.01933, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0157 - acc: 0.9764 - val_loss: 0.0193 - val_acc: 0.9710\n",
      "Epoch 163/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9763Epoch 00163: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0158 - acc: 0.9762 - val_loss: 0.0201 - val_acc: 0.9699\n",
      "Epoch 164/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9763Epoch 00164: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0158 - acc: 0.9763 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 165/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9759Epoch 00165: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0161 - acc: 0.9759 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 166/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9766Epoch 00166: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0156 - acc: 0.9766 - val_loss: 0.0195 - val_acc: 0.9708\n",
      "Epoch 167/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9765Epoch 00167: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0156 - acc: 0.9765 - val_loss: 0.0199 - val_acc: 0.9702\n",
      "Epoch 168/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9766Epoch 00168: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0156 - acc: 0.9766 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 169/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9765Epoch 00169: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0157 - acc: 0.9765 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 170/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9767Epoch 00170: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0156 - acc: 0.9767 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 171/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9767Epoch 00171: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0155 - acc: 0.9767 - val_loss: 0.0200 - val_acc: 0.9700\n",
      "Epoch 172/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9764Epoch 00172: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0157 - acc: 0.9764 - val_loss: 0.0198 - val_acc: 0.9703\n",
      "Epoch 173/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9768Epoch 00173: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0155 - acc: 0.9768 - val_loss: 0.0199 - val_acc: 0.9702\n",
      "Epoch 174/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9766Epoch 00174: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0156 - acc: 0.9766 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 175/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9767Epoch 00175: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0156 - acc: 0.9766 - val_loss: 0.0197 - val_acc: 0.9705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9768Epoch 00176: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0155 - acc: 0.9768 - val_loss: 0.0221 - val_acc: 0.9669\n",
      "Epoch 177/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9770Epoch 00177: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0154 - acc: 0.9769 - val_loss: 0.0205 - val_acc: 0.9692\n",
      "Epoch 178/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9768Epoch 00178: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0155 - acc: 0.9768 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 179/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9769Epoch 00179: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0154 - acc: 0.9769 - val_loss: 0.0200 - val_acc: 0.9700\n",
      "Epoch 180/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9770Epoch 00180: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0153 - acc: 0.9770 - val_loss: 0.0194 - val_acc: 0.9710\n",
      "Epoch 181/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9770Epoch 00181: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0153 - acc: 0.9770 - val_loss: 0.0195 - val_acc: 0.9708\n",
      "Epoch 182/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9767Epoch 00182: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0156 - acc: 0.9767 - val_loss: 0.0247 - val_acc: 0.9630\n",
      "Epoch 183/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9771Epoch 00183: val_loss improved from 0.01933 to 0.01926, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0153 - acc: 0.9771 - val_loss: 0.0193 - val_acc: 0.9711\n",
      "Epoch 184/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9771Epoch 00184: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0153 - acc: 0.9771 - val_loss: 0.0211 - val_acc: 0.9683\n",
      "Epoch 185/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9772Epoch 00185: val_loss improved from 0.01926 to 0.01925, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0152 - acc: 0.9772 - val_loss: 0.0192 - val_acc: 0.9711\n",
      "Epoch 186/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9771Epoch 00186: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0153 - acc: 0.9771 - val_loss: 0.0193 - val_acc: 0.9711\n",
      "Epoch 187/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9773Epoch 00187: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0152 - acc: 0.9773 - val_loss: 0.0195 - val_acc: 0.9707\n",
      "Epoch 188/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9773Epoch 00188: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0152 - acc: 0.9773 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 189/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9772Epoch 00189: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0152 - acc: 0.9772 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 190/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9773Epoch 00190: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0151 - acc: 0.9773 - val_loss: 0.0207 - val_acc: 0.9689\n",
      "Epoch 191/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9773Epoch 00191: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0151 - acc: 0.9774 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 192/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9774Epoch 00192: val_loss did not improve\n",
      "5976/5976 [==============================] - 1103s 185ms/step - loss: 0.0151 - acc: 0.9774 - val_loss: 0.0223 - val_acc: 0.9666\n",
      "Epoch 193/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9773Epoch 00193: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0151 - acc: 0.9773 - val_loss: 0.0200 - val_acc: 0.9700\n",
      "Epoch 194/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9775Epoch 00194: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0150 - acc: 0.9775 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 195/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0151 - acc: 0.9774Epoch 00195: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0151 - acc: 0.9774 - val_loss: 0.0196 - val_acc: 0.9706\n",
      "Epoch 196/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9775Epoch 00196: val_loss improved from 0.01925 to 0.01904, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0150 - acc: 0.9775 - val_loss: 0.0190 - val_acc: 0.9715\n",
      "Epoch 197/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9776Epoch 00197: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0150 - acc: 0.9776 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 198/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9775Epoch 00198: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0150 - acc: 0.9775 - val_loss: 0.0202 - val_acc: 0.9698\n",
      "Epoch 199/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9776Epoch 00199: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0149 - acc: 0.9776 - val_loss: 0.0194 - val_acc: 0.9710\n",
      "Epoch 200/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9776Epoch 00200: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0149 - acc: 0.9776 - val_loss: 0.0206 - val_acc: 0.9691\n",
      "Epoch 201/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9777Epoch 00201: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0149 - acc: 0.9777 - val_loss: 0.0198 - val_acc: 0.9704\n",
      "Epoch 202/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9776Epoch 00202: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0149 - acc: 0.9776 - val_loss: 0.0209 - val_acc: 0.9687\n",
      "Epoch 203/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9778Epoch 00203: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0148 - acc: 0.9778 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 204/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9777Epoch 00204: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0149 - acc: 0.9777 - val_loss: 0.0202 - val_acc: 0.9697\n",
      "Epoch 205/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9778Epoch 00205: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0148 - acc: 0.9778 - val_loss: 0.0249 - val_acc: 0.9627\n",
      "Epoch 206/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0149 - acc: 0.9777Epoch 00206: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0149 - acc: 0.9777 - val_loss: 0.0197 - val_acc: 0.9705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9779Epoch 00207: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0147 - acc: 0.9779 - val_loss: 0.0198 - val_acc: 0.9704\n",
      "Epoch 208/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9779Epoch 00208: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0148 - acc: 0.9779 - val_loss: 0.0198 - val_acc: 0.9704\n",
      "Epoch 209/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9779Epoch 00209: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0147 - acc: 0.9780 - val_loss: 0.0195 - val_acc: 0.9708\n",
      "Epoch 210/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9775Epoch 00210: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0150 - acc: 0.9775 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 211/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9780Epoch 00211: val_loss improved from 0.01904 to 0.01900, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0147 - acc: 0.9780 - val_loss: 0.0190 - val_acc: 0.9715\n",
      "Epoch 212/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9781Epoch 00212: val_loss improved from 0.01900 to 0.01893, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9781 - val_loss: 0.0189 - val_acc: 0.9716\n",
      "Epoch 213/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0148 - acc: 0.9778Epoch 00213: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0148 - acc: 0.9778 - val_loss: 0.0192 - val_acc: 0.9712\n",
      "Epoch 214/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9780Epoch 00214: val_loss improved from 0.01893 to 0.01872, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9781 - val_loss: 0.0187 - val_acc: 0.9719\n",
      "Epoch 215/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9781Epoch 00215: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9781 - val_loss: 0.0214 - val_acc: 0.9678\n",
      "Epoch 216/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9781Epoch 00216: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9781 - val_loss: 0.0207 - val_acc: 0.9690\n",
      "Epoch 217/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9782Epoch 00217: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0145 - acc: 0.9782 - val_loss: 0.0196 - val_acc: 0.9706\n",
      "Epoch 218/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9782Epoch 00218: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9782 - val_loss: 0.0192 - val_acc: 0.9713\n",
      "Epoch 219/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9782Epoch 00219: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9782 - val_loss: 0.0193 - val_acc: 0.9710\n",
      "Epoch 220/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9782Epoch 00220: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0145 - acc: 0.9782 - val_loss: 0.0193 - val_acc: 0.9710\n",
      "Epoch 221/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9784Epoch 00221: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0144 - acc: 0.9784 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 222/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9782Epoch 00222: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0145 - acc: 0.9782 - val_loss: 0.0193 - val_acc: 0.9711\n",
      "Epoch 223/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9783Epoch 00223: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0144 - acc: 0.9784 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 224/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9784Epoch 00224: val_loss improved from 0.01872 to 0.01854, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0144 - acc: 0.9784 - val_loss: 0.0185 - val_acc: 0.9722\n",
      "Epoch 225/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9785Epoch 00225: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0144 - acc: 0.9785 - val_loss: 0.0191 - val_acc: 0.9713\n",
      "Epoch 226/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9779Epoch 00226: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0147 - acc: 0.9779 - val_loss: 0.0198 - val_acc: 0.9703\n",
      "Epoch 227/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9783Epoch 00227: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0145 - acc: 0.9783 - val_loss: 0.0191 - val_acc: 0.9714\n",
      "Epoch 228/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9785Epoch 00228: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0143 - acc: 0.9785 - val_loss: 0.0191 - val_acc: 0.9714\n",
      "Epoch 229/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9786Epoch 00229: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0143 - acc: 0.9786 - val_loss: 0.0194 - val_acc: 0.9709\n",
      "Epoch 230/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9785Epoch 00230: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0143 - acc: 0.9785 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 231/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9782Epoch 00231: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0146 - acc: 0.9782 - val_loss: 0.0188 - val_acc: 0.9718\n",
      "Epoch 232/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9787Epoch 00232: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0142 - acc: 0.9787 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 233/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9787Epoch 00233: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0142 - acc: 0.9787 - val_loss: 0.0198 - val_acc: 0.9703\n",
      "Epoch 234/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9787Epoch 00234: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0142 - acc: 0.9787 - val_loss: 0.0191 - val_acc: 0.9714\n",
      "Epoch 235/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9787Epoch 00235: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0142 - acc: 0.9787 - val_loss: 0.0191 - val_acc: 0.9714\n",
      "Epoch 236/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9786Epoch 00236: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0143 - acc: 0.9786 - val_loss: 0.0189 - val_acc: 0.9716\n",
      "Epoch 237/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9787Epoch 00237: val_loss did not improve\n",
      "5976/5976 [==============================] - 1106s 185ms/step - loss: 0.0142 - acc: 0.9787 - val_loss: 0.0191 - val_acc: 0.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0142 - acc: 0.9788Epoch 00238: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0142 - acc: 0.9788 - val_loss: 0.0188 - val_acc: 0.9718\n",
      "Epoch 239/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9788Epoch 00239: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0141 - acc: 0.9788 - val_loss: 0.0208 - val_acc: 0.9689\n",
      "Epoch 240/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9789Epoch 00240: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0141 - acc: 0.9789 - val_loss: 0.0189 - val_acc: 0.9716\n",
      "Epoch 241/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9789Epoch 00241: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0141 - acc: 0.9789 - val_loss: 0.0199 - val_acc: 0.9701\n",
      "Epoch 242/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9788Epoch 00242: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0141 - acc: 0.9789 - val_loss: 0.0211 - val_acc: 0.9684\n",
      "Epoch 243/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9789Epoch 00243: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0141 - acc: 0.9789 - val_loss: 0.0187 - val_acc: 0.9719\n",
      "Epoch 244/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9783Epoch 00244: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0145 - acc: 0.9783 - val_loss: 0.0192 - val_acc: 0.9712\n",
      "Epoch 245/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9791Epoch 00245: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0139 - acc: 0.9791 - val_loss: 0.0205 - val_acc: 0.9692\n",
      "Epoch 246/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9790Epoch 00246: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0140 - acc: 0.9790 - val_loss: 0.0195 - val_acc: 0.9707\n",
      "Epoch 247/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9791Epoch 00247: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0140 - acc: 0.9791 - val_loss: 0.0190 - val_acc: 0.9716\n",
      "Epoch 248/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9791Epoch 00248: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0139 - acc: 0.9791 - val_loss: 0.0203 - val_acc: 0.9696\n",
      "Epoch 249/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0140 - acc: 0.9791Epoch 00249: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0140 - acc: 0.9791 - val_loss: 0.0187 - val_acc: 0.9720\n",
      "Epoch 250/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9792Epoch 00250: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0139 - acc: 0.9792 - val_loss: 0.0199 - val_acc: 0.9702\n",
      "Epoch 251/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9789Epoch 00251: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0141 - acc: 0.9789 - val_loss: 0.0188 - val_acc: 0.9718\n",
      "Epoch 252/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9792Epoch 00252: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0139 - acc: 0.9792 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 253/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9792Epoch 00253: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0138 - acc: 0.9792 - val_loss: 0.0187 - val_acc: 0.9720\n",
      "Epoch 254/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9792Epoch 00254: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0139 - acc: 0.9792 - val_loss: 0.0200 - val_acc: 0.9701\n",
      "Epoch 255/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9793Epoch 00255: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0138 - acc: 0.9793 - val_loss: 0.0193 - val_acc: 0.9711\n",
      "Epoch 256/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9792Epoch 00256: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0138 - acc: 0.9793 - val_loss: 0.0186 - val_acc: 0.9721\n",
      "Epoch 257/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9794Epoch 00257: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0137 - acc: 0.9794 - val_loss: 0.0186 - val_acc: 0.9722\n",
      "Epoch 258/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0139 - acc: 0.9792Epoch 00258: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0139 - acc: 0.9792 - val_loss: 0.0196 - val_acc: 0.9707\n",
      "Epoch 259/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9794Epoch 00259: val_loss improved from 0.01854 to 0.01836, saving model to deeplabv3+withresnet50new.h5\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0138 - acc: 0.9794 - val_loss: 0.0184 - val_acc: 0.9725\n",
      "Epoch 260/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9795Epoch 00260: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0137 - acc: 0.9795 - val_loss: 0.0195 - val_acc: 0.9707\n",
      "Epoch 261/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9795Epoch 00261: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0137 - acc: 0.9795 - val_loss: 0.0199 - val_acc: 0.9702\n",
      "Epoch 262/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9795Epoch 00262: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0137 - acc: 0.9795 - val_loss: 0.0215 - val_acc: 0.9678\n",
      "Epoch 263/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9795Epoch 00263: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0137 - acc: 0.9795 - val_loss: 0.0208 - val_acc: 0.9688\n",
      "Epoch 264/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9796Epoch 00264: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0136 - acc: 0.9796 - val_loss: 0.0201 - val_acc: 0.9699\n",
      "Epoch 265/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9795Epoch 00265: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0137 - acc: 0.9795 - val_loss: 0.0197 - val_acc: 0.9705\n",
      "Epoch 266/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9797Epoch 00266: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0136 - acc: 0.9797 - val_loss: 0.0188 - val_acc: 0.9718\n",
      "Epoch 267/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9796Epoch 00267: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0136 - acc: 0.9796 - val_loss: 0.0193 - val_acc: 0.9711\n",
      "Epoch 268/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9796Epoch 00268: val_loss did not improve\n",
      "5976/5976 [==============================] - 1104s 185ms/step - loss: 0.0136 - acc: 0.9796 - val_loss: 0.0188 - val_acc: 0.9718\n",
      "Epoch 269/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9796Epoch 00269: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0136 - acc: 0.9796 - val_loss: 0.0190 - val_acc: 0.9715\n",
      "Epoch 270/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0134 - acc: 0.9798Epoch 00270: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0134 - acc: 0.9799 - val_loss: 0.0195 - val_acc: 0.9708\n",
      "Epoch 271/300\n",
      "5973/5976 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9796Epoch 00271: val_loss did not improve\n",
      "5976/5976 [==============================] - 1105s 185ms/step - loss: 0.0136 - acc: 0.9797 - val_loss: 0.0189 - val_acc: 0.9716\n",
      "Epoch 272/300\n",
      "3153/5976 [==============>...............] - ETA: 8:26 - loss: 0.0129 - acc: 0.9806"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-99661804a65b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeplabv3plus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = deeplabv3plus.fit(X_train, copied, batch_size=3, epochs=300,callbacks=callbacks,validation_data=(X_val,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deeplabv3plus.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_frame(frame, percent=75):\n",
    "    import cv2\n",
    "    width = int(frame.shape[1] * percent/ 100)\n",
    "    height = int(frame.shape[0] * percent/ 100)\n",
    "    dim = (width, height)\n",
    "    return cv2.resize(frame, dim, interpolation =cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def evaluation(y,stage,model_name,pred,pred2):#,pred2\n",
    "  \n",
    "    pred1=(pred+1.0)/2.0\n",
    "    gt = (y+1.0)/2.0\n",
    "    pr=pred1.reshape((pred1.shape[0],384,  512, 3)).argmax(axis=3)\n",
    "    tp = np.zeros(3)\n",
    "    fp = np.zeros(3)\n",
    "    fn = np.zeros(3)\n",
    "\n",
    "    n_pixels = np.zeros(3)\n",
    "    gt = gt.argmax(-1)\n",
    "    pr = pr.flatten()\n",
    "    gt = gt.flatten()\n",
    "    for cl_i in range(3):\n",
    "        tp[ cl_i ] += np.sum( (pr == cl_i) * (gt == cl_i) )\n",
    "        fp[ cl_i ] += np.sum( (pr == cl_i) * ((gt != cl_i)) )\n",
    "        fn[ cl_i ] += np.sum( (pr != cl_i) * ((gt == cl_i)) )\n",
    "        n_pixels[ cl_i ] += np.sum( gt == cl_i  )\n",
    "\n",
    "    cl_wise_score = tp / ( tp + fp + fn + 0.000000000001 )\n",
    "    n_pixels_norm = n_pixels /  np.sum(n_pixels)\n",
    "    frequency_weighted_IU = np.sum(cl_wise_score*n_pixels_norm)\n",
    "    mean_IU = np.mean(cl_wise_score)\n",
    "    dice = 2*tp/(2*tp+fp+fn)\n",
    "    print(dice,'Dice class wise')\n",
    "    print(np.mean(dice),'mean Dice')\n",
    "    print(\"frequency_weighted_IU\",frequency_weighted_IU , \"mean_IU\",mean_IU , \"class_wise_IU\",cl_wise_score)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabv3plus.load_weights('deeplabv3+withresnet50.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
