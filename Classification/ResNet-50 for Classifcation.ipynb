{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep 28 13:51:50 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 23%   31C    P8     9W / 250W |    154MiB / 11176MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:06:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8     8W / 250W |    139MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8    N/A /  N/A |    139MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1446      G   /usr/lib/xorg/Xorg                 14MiB |\n",
      "|    0   N/A  N/A      9696      C   ...envs/tf_gpu/bin/python3.6      135MiB |\n",
      "|    1   N/A  N/A      9696      C   ...envs/tf_gpu/bin/python3.6      135MiB |\n",
      "|    2   N/A  N/A      9696      C   ...envs/tf_gpu/bin/python3.6      135MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # check if the GPU is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # uncomment if there is GPU\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "import glob\n",
    "import re\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def one_side_pad(x):\n",
    "    x = ZeroPadding2D((1, 1))(x)\n",
    "    x = Lambda(lambda x: x[:, :-1, :-1, :])(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at\n",
    "                     main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1),name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1),name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block,\n",
    "               strides=(2, 2)):\n",
    "\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size, padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1),name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_resnet50_encoder(input_height=256,  input_width=256, classes=1000):\n",
    "\n",
    "    assert input_height % 32 == 0\n",
    "    assert input_width % 32 == 0\n",
    "\n",
    " \n",
    "    img_input = Input(shape=(input_height, input_width, 3))\n",
    "    x = ZeroPadding2D((3, 3))(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
    "    f1 = x\n",
    "\n",
    "    x = BatchNormalization(name='bn_conv1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((3, 3),strides=(2, 2))(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "    f2 = one_side_pad(x)\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
    "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
    "    f3 = x\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
    "    f4 = x\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "    f5 = x\n",
    "\n",
    "    x = AveragePooling2D(name='avg_pool')(x)\n",
    "    flatten1 = Flatten()(x)\n",
    "    \n",
    "    dense = Dense(units=classes, kernel_initializer=\"he_normal\",activation=\"softmax\")(flatten1)\n",
    "    model= Model(inputs = img_input, outputs = dense)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_resnet50_encoder(input_height=256,  input_width=256, classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.0001, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5656 images belonging to 5 classes.\n",
      "Found 163 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=40,\n",
    "        vertical_flip=True,\n",
    "        horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"/home/sami/Vincent/train\",\n",
    "        target_size=(256 , 256),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '/home/sami/Vincent/pdata/validation',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    ModelCheckpoint('resnet50_aug.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=2000,epochs=3,callbacks=callbacks,\n",
    "                              validation_data=validation_generator,validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:958: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:680: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:945: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2378: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:159: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:164: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:173: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:182: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sami/anaconda3/envs/tf_gpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 1.4090 - acc: 0.5846Epoch 00001: val_loss improved from inf to 1.45875, saving model to resnet_wo_aug.h5\n",
      "2000/2000 [==============================] - 1185s 593ms/step - loss: 1.4086 - acc: 0.5847 - val_loss: 1.4587 - val_acc: 0.4478\n",
      "Epoch 2/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.8136 - acc: 0.7767Epoch 00002: val_loss did not improve\n",
      "2000/2000 [==============================] - 1139s 569ms/step - loss: 0.8135 - acc: 0.7767 - val_loss: 2.1477 - val_acc: 0.4110\n",
      "Epoch 3/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.2054 - acc: 0.9424Epoch 00003: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 567ms/step - loss: 0.2053 - acc: 0.9425 - val_loss: 2.1163 - val_acc: 0.5275\n",
      "Epoch 4/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.9627Epoch 00004: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.1460 - acc: 0.9627 - val_loss: 3.3659 - val_acc: 0.3617\n",
      "Epoch 5/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.9659Epoch 00005: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.1394 - acc: 0.9660 - val_loss: 4.5053 - val_acc: 0.4480\n",
      "Epoch 6/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1121 - acc: 0.9731Epoch 00006: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.1121 - acc: 0.9731 - val_loss: 3.0702 - val_acc: 0.4292\n",
      "Epoch 7/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9790Epoch 00007: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0903 - acc: 0.9791 - val_loss: 4.5289 - val_acc: 0.3925\n",
      "Epoch 8/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0930 - acc: 0.9777Epoch 00008: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0930 - acc: 0.9777 - val_loss: 3.0182 - val_acc: 0.4356\n",
      "Epoch 9/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9861Epoch 00009: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0591 - acc: 0.9861 - val_loss: 3.7061 - val_acc: 0.5030\n",
      "Epoch 10/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0852 - acc: 0.9802Epoch 00010: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0851 - acc: 0.9803 - val_loss: 3.9911 - val_acc: 0.4783\n",
      "Epoch 11/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9892Epoch 00011: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0409 - acc: 0.9892 - val_loss: 4.0278 - val_acc: 0.4787\n",
      "Epoch 12/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0701 - acc: 0.9831Epoch 00012: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0701 - acc: 0.9831 - val_loss: 4.5088 - val_acc: 0.4599\n",
      "Epoch 13/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9896Epoch 00013: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0403 - acc: 0.9896 - val_loss: 3.4314 - val_acc: 0.5275\n",
      "Epoch 14/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9861Epoch 00014: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0527 - acc: 0.9861 - val_loss: 3.8189 - val_acc: 0.4847\n",
      "Epoch 15/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9891Epoch 00015: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0416 - acc: 0.9891 - val_loss: 4.0202 - val_acc: 0.4906\n",
      "Epoch 16/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9875Epoch 00016: val_loss did not improve\n",
      "2000/2000 [==============================] - 1132s 566ms/step - loss: 0.0451 - acc: 0.9875 - val_loss: 3.9474 - val_acc: 0.5152\n",
      "Epoch 17/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9880Epoch 00017: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0473 - acc: 0.9880 - val_loss: 4.4298 - val_acc: 0.4662\n",
      "Epoch 18/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9915Epoch 00018: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0284 - acc: 0.9915 - val_loss: 3.9365 - val_acc: 0.4907\n",
      "Epoch 19/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0104 - acc: 0.9960Epoch 00019: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0104 - acc: 0.9960 - val_loss: 4.3769 - val_acc: 0.4971\n",
      "Epoch 20/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9847Epoch 00020: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0673 - acc: 0.9847 - val_loss: 4.7176 - val_acc: 0.4171\n",
      "Epoch 21/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0123 - acc: 0.9958Epoch 00021: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0123 - acc: 0.9958 - val_loss: 4.9884 - val_acc: 0.4908\n",
      "Epoch 22/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0719 - acc: 0.9839Epoch 00022: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0719 - acc: 0.9839 - val_loss: 4.5621 - val_acc: 0.4234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9942Epoch 00023: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0155 - acc: 0.9942 - val_loss: 4.4050 - val_acc: 0.4538\n",
      "Epoch 24/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9890Epoch 00024: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0374 - acc: 0.9890 - val_loss: 4.3628 - val_acc: 0.4539\n",
      "Epoch 25/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9950Epoch 00025: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0136 - acc: 0.9950 - val_loss: 4.6773 - val_acc: 0.4540\n",
      "Epoch 26/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9894Epoch 00026: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0399 - acc: 0.9894 - val_loss: 4.0443 - val_acc: 0.4048\n",
      "Epoch 27/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9950Epoch 00027: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0137 - acc: 0.9950 - val_loss: 4.4722 - val_acc: 0.4109\n",
      "Epoch 28/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9905Epoch 00028: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.0307 - acc: 0.9905 - val_loss: 4.1755 - val_acc: 0.4112\n",
      "Epoch 29/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0127 - acc: 0.9951Epoch 00029: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0127 - acc: 0.9951 - val_loss: 4.4376 - val_acc: 0.4479\n",
      "Epoch 30/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9915Epoch 00030: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0320 - acc: 0.9915 - val_loss: 4.1907 - val_acc: 0.4354\n",
      "Epoch 31/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9943Epoch 00031: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0162 - acc: 0.9942 - val_loss: 3.6472 - val_acc: 0.4663\n",
      "Epoch 32/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9935Epoch 00032: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.0192 - acc: 0.9935 - val_loss: 4.2481 - val_acc: 0.4969\n",
      "Epoch 33/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0132 - acc: 0.9949Epoch 00033: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0132 - acc: 0.9949 - val_loss: 4.6036 - val_acc: 0.4846\n",
      "Epoch 34/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0241 - acc: 0.9927Epoch 00034: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.0241 - acc: 0.9927 - val_loss: 4.6550 - val_acc: 0.4358\n",
      "Epoch 35/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9958Epoch 00035: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0100 - acc: 0.9958 - val_loss: 4.9241 - val_acc: 0.4662\n",
      "Epoch 36/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0259 - acc: 0.9921Epoch 00036: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0259 - acc: 0.9921 - val_loss: 4.1756 - val_acc: 0.4602\n",
      "Epoch 37/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0138 - acc: 0.9947Epoch 00037: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 567ms/step - loss: 0.0138 - acc: 0.9947 - val_loss: 5.4909 - val_acc: 0.4541\n",
      "Epoch 38/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9932Epoch 00038: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0193 - acc: 0.9932 - val_loss: 5.2712 - val_acc: 0.4111\n",
      "Epoch 39/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9963Epoch 00039: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 566ms/step - loss: 0.0081 - acc: 0.9963 - val_loss: 5.3629 - val_acc: 0.4537\n",
      "Epoch 40/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9938Epoch 00040: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0177 - acc: 0.9938 - val_loss: 4.1109 - val_acc: 0.4968\n",
      "Epoch 41/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9930Epoch 00041: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0201 - acc: 0.9930 - val_loss: 4.0202 - val_acc: 0.4236\n",
      "Epoch 42/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9962Epoch 00042: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0074 - acc: 0.9962 - val_loss: 4.6653 - val_acc: 0.4600\n",
      "Epoch 43/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9925Epoch 00043: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0238 - acc: 0.9925 - val_loss: 4.7883 - val_acc: 0.4111\n",
      "Epoch 44/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9963Epoch 00044: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 567ms/step - loss: 0.0074 - acc: 0.9963 - val_loss: 5.2522 - val_acc: 0.4418\n",
      "Epoch 45/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9938Epoch 00045: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 567ms/step - loss: 0.0179 - acc: 0.9938 - val_loss: 3.5705 - val_acc: 0.4723\n",
      "Epoch 46/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9964Epoch 00046: val_loss did not improve\n",
      "2000/2000 [==============================] - 1133s 567ms/step - loss: 0.0093 - acc: 0.9964 - val_loss: 4.2487 - val_acc: 0.4909\n",
      "Epoch 47/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0192 - acc: 0.9940Epoch 00047: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0192 - acc: 0.9940 - val_loss: 4.2090 - val_acc: 0.4663\n",
      "Epoch 48/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9965Epoch 00048: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0069 - acc: 0.9965 - val_loss: 4.4818 - val_acc: 0.4724\n",
      "Epoch 49/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9928Epoch 00049: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0225 - acc: 0.9928 - val_loss: 3.7922 - val_acc: 0.5029\n",
      "Epoch 50/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9968Epoch 00050: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0065 - acc: 0.9968 - val_loss: 4.4538 - val_acc: 0.4788\n",
      "Epoch 51/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0170 - acc: 0.9941Epoch 00051: val_loss did not improve\n",
      "2000/2000 [==============================] - 1134s 567ms/step - loss: 0.0170 - acc: 0.9941 - val_loss: 3.9917 - val_acc: 0.4722\n",
      "Epoch 52/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9968Epoch 00052: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 567ms/step - loss: 0.0075 - acc: 0.9968 - val_loss: 4.3490 - val_acc: 0.4600\n",
      "Epoch 53/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0163 - acc: 0.9944Epoch 00053: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0163 - acc: 0.9944 - val_loss: 4.6039 - val_acc: 0.4356\n",
      "Epoch 54/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9967Epoch 00054: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0063 - acc: 0.9967 - val_loss: 4.7335 - val_acc: 0.4541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0172 - acc: 0.9939Epoch 00055: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0172 - acc: 0.9939 - val_loss: 4.8622 - val_acc: 0.4663\n",
      "Epoch 56/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0141 - acc: 0.9947Epoch 00056: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0141 - acc: 0.9947 - val_loss: 4.7254 - val_acc: 0.4415\n",
      "Epoch 57/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9958Epoch 00057: val_loss did not improve\n",
      "2000/2000 [==============================] - 1135s 568ms/step - loss: 0.0096 - acc: 0.9958 - val_loss: 5.6059 - val_acc: 0.4417\n",
      "Epoch 58/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9969Epoch 00058: val_loss did not improve\n",
      "2000/2000 [==============================] - 1138s 569ms/step - loss: 0.0062 - acc: 0.9969 - val_loss: 5.5053 - val_acc: 0.4417\n",
      "Epoch 59/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0175 - acc: 0.9943Epoch 00059: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0175 - acc: 0.9943 - val_loss: 4.5799 - val_acc: 0.4664\n",
      "Epoch 60/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9967Epoch 00060: val_loss did not improve\n",
      "2000/2000 [==============================] - 1139s 569ms/step - loss: 0.0062 - acc: 0.9967 - val_loss: 4.7519 - val_acc: 0.4725\n",
      "Epoch 61/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0158 - acc: 0.9945Epoch 00061: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0158 - acc: 0.9945 - val_loss: 4.3296 - val_acc: 0.4417\n",
      "Epoch 62/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9967Epoch 00062: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0070 - acc: 0.9967 - val_loss: 4.5973 - val_acc: 0.4722\n",
      "Epoch 63/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0153 - acc: 0.9942Epoch 00063: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0153 - acc: 0.9941 - val_loss: 4.2864 - val_acc: 0.4846\n",
      "Epoch 64/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9954Epoch 00064: val_loss did not improve\n",
      "2000/2000 [==============================] - 1141s 570ms/step - loss: 0.0119 - acc: 0.9954 - val_loss: 4.2306 - val_acc: 0.4787\n",
      "Epoch 65/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9965Epoch 00065: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0099 - acc: 0.9964 - val_loss: 4.5773 - val_acc: 0.5030\n",
      "Epoch 66/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0150 - acc: 0.9945Epoch 00066: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0150 - acc: 0.9945 - val_loss: 4.6105 - val_acc: 0.4724\n",
      "Epoch 67/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9957Epoch 00067: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 568ms/step - loss: 0.0098 - acc: 0.9957 - val_loss: 4.9350 - val_acc: 0.4785\n",
      "Epoch 68/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9965Epoch 00068: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0065 - acc: 0.9965 - val_loss: 4.6577 - val_acc: 0.4786\n",
      "Epoch 69/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9957Epoch 00069: val_loss did not improve\n",
      "2000/2000 [==============================] - 1137s 569ms/step - loss: 0.0110 - acc: 0.9958 - val_loss: 4.3167 - val_acc: 0.4910\n",
      "Epoch 70/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9954Epoch 00070: val_loss did not improve\n",
      "2000/2000 [==============================] - 1140s 570ms/step - loss: 0.0118 - acc: 0.9954 - val_loss: 4.6101 - val_acc: 0.4968\n",
      "Epoch 71/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9966Epoch 00071: val_loss did not improve\n",
      "2000/2000 [==============================] - 1139s 569ms/step - loss: 0.0074 - acc: 0.9966 - val_loss: 4.6986 - val_acc: 0.5154\n",
      "Epoch 72/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9958Epoch 00072: val_loss did not improve\n",
      "2000/2000 [==============================] - 1138s 569ms/step - loss: 0.0090 - acc: 0.9958 - val_loss: 4.8490 - val_acc: 0.4356\n",
      "Epoch 73/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9965Epoch 00073: val_loss did not improve\n",
      "2000/2000 [==============================] - 1136s 568ms/step - loss: 0.0068 - acc: 0.9965 - val_loss: 4.9067 - val_acc: 0.4725\n",
      "Epoch 74/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0154 - acc: 0.9943Epoch 00074: val_loss did not improve\n",
      "2000/2000 [==============================] - 1138s 569ms/step - loss: 0.0154 - acc: 0.9943 - val_loss: 4.0580 - val_acc: 0.4847\n",
      "Epoch 75/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9969Epoch 00075: val_loss did not improve\n",
      "2000/2000 [==============================] - 1142s 571ms/step - loss: 0.0066 - acc: 0.9969 - val_loss: 4.7386 - val_acc: 0.4968\n",
      "Epoch 76/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9956Epoch 00076: val_loss did not improve\n",
      "2000/2000 [==============================] - 1138s 569ms/step - loss: 0.0101 - acc: 0.9956 - val_loss: 4.9429 - val_acc: 0.4600\n",
      "Epoch 77/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9964Epoch 00077: val_loss did not improve\n",
      "2000/2000 [==============================] - 1132s 566ms/step - loss: 0.0082 - acc: 0.9964 - val_loss: 4.7224 - val_acc: 0.4787\n",
      "Epoch 78/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9962Epoch 00078: val_loss did not improve\n",
      "2000/2000 [==============================] - 1132s 566ms/step - loss: 0.0083 - acc: 0.9962 - val_loss: 4.5689 - val_acc: 0.4847\n",
      "Epoch 79/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9958Epoch 00079: val_loss did not improve\n",
      "2000/2000 [==============================] - 1131s 566ms/step - loss: 0.0092 - acc: 0.9958 - val_loss: 4.6312 - val_acc: 0.4907\n",
      "Epoch 80/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9969Epoch 00080: val_loss did not improve\n",
      "2000/2000 [==============================] - 1131s 566ms/step - loss: 0.0061 - acc: 0.9969 - val_loss: 5.2242 - val_acc: 0.4666\n",
      "Epoch 81/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9957Epoch 00081: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0114 - acc: 0.9958 - val_loss: 4.4049 - val_acc: 0.4663\n",
      "Epoch 82/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0098 - acc: 0.9965Epoch 00082: val_loss did not improve\n",
      "2000/2000 [==============================] - 1132s 566ms/step - loss: 0.0098 - acc: 0.9965 - val_loss: 4.8176 - val_acc: 0.4725\n",
      "Epoch 83/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9954Epoch 00083: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0131 - acc: 0.9954 - val_loss: 5.1198 - val_acc: 0.4603\n",
      "Epoch 84/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9966Epoch 00084: val_loss did not improve\n",
      "2000/2000 [==============================] - 1131s 566ms/step - loss: 0.0090 - acc: 0.9966 - val_loss: 5.3378 - val_acc: 0.4724\n",
      "Epoch 85/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9965Epoch 00085: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0087 - acc: 0.9965 - val_loss: 5.4089 - val_acc: 0.4725\n",
      "Epoch 86/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0167 - acc: 0.9947Epoch 00086: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0167 - acc: 0.9947 - val_loss: 4.1264 - val_acc: 0.4417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9967Epoch 00087: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0114 - acc: 0.9967 - val_loss: 4.7584 - val_acc: 0.4603\n",
      "Epoch 88/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0144 - acc: 0.9961Epoch 00088: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0144 - acc: 0.9961 - val_loss: 7.4663 - val_acc: 0.3251\n",
      "Epoch 89/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9960Epoch 00089: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0124 - acc: 0.9960 - val_loss: 4.8291 - val_acc: 0.5093\n",
      "Epoch 90/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9958Epoch 00090: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 4.2698 - val_acc: 0.4969\n",
      "Epoch 91/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0129 - acc: 0.9959Epoch 00091: val_loss did not improve\n",
      "2000/2000 [==============================] - 1129s 565ms/step - loss: 0.0129 - acc: 0.9959 - val_loss: 4.5065 - val_acc: 0.4907\n",
      "Epoch 92/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9968Epoch 00092: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0091 - acc: 0.9968 - val_loss: 4.9043 - val_acc: 0.4662\n",
      "Epoch 93/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0152 - acc: 0.9954Epoch 00093: val_loss did not improve\n",
      "2000/2000 [==============================] - 1131s 565ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 5.0466 - val_acc: 0.4602\n",
      "Epoch 94/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9965Epoch 00094: val_loss did not improve\n",
      "2000/2000 [==============================] - 1129s 564ms/step - loss: 0.0124 - acc: 0.9965 - val_loss: 4.6642 - val_acc: 0.4663\n",
      "Epoch 95/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0113 - acc: 0.9964Epoch 00095: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0113 - acc: 0.9964 - val_loss: 4.9687 - val_acc: 0.4726\n",
      "Epoch 96/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9952Epoch 00096: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0173 - acc: 0.9952 - val_loss: 4.7029 - val_acc: 0.5033\n",
      "Epoch 97/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9960Epoch 00097: val_loss did not improve\n",
      "2000/2000 [==============================] - 1129s 565ms/step - loss: 0.0107 - acc: 0.9960 - val_loss: 5.2654 - val_acc: 0.4598\n",
      "Epoch 98/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0071 - acc: 0.9965Epoch 00098: val_loss did not improve\n",
      "2000/2000 [==============================] - 1129s 565ms/step - loss: 0.0071 - acc: 0.9965 - val_loss: 5.3174 - val_acc: 0.5092\n",
      "Epoch 99/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9959Epoch 00099: val_loss did not improve\n",
      "2000/2000 [==============================] - 1129s 564ms/step - loss: 0.0094 - acc: 0.9958 - val_loss: 4.8689 - val_acc: 0.4725\n",
      "Epoch 100/100\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9966Epoch 00100: val_loss did not improve\n",
      "2000/2000 [==============================] - 1130s 565ms/step - loss: 0.0039 - acc: 0.9966 - val_loss: 5.0156 - val_acc: 0.4788\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    ModelCheckpoint('resnet_wo_aug.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "history = model.fit_generator(train_generator,steps_per_epoch=2000,epochs=100,callbacks=callbacks,\n",
    "                              validation_data=validation_generator,validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hist_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'history_training.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "plt.show()\n",
    "plt.savefig(model.name+'_accuracy.png')\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"loss\",\"Validation Loss\"])\n",
    "plt.show()\n",
    "plt.savefig(model.name+'_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('resnet50_aug.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 2s 19ms/step\n",
      "[1.611379639157709, 0.2358490566037736]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=np.round(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = np.asarray(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Labels\n",
      "[3 3 3 3 1]\n",
      "Validation Prediction\n",
      "[3 3 3 3 0]\n",
      "Confusion Matrix\n",
      "[[ 4  9  0  1  0]\n",
      " [ 3 16  0  2  3]\n",
      " [ 0  1  2 16  0]\n",
      " [ 1  2  2 19  2]\n",
      " [ 3  6  1  7  6]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cbb       0.36      0.29      0.32        14\n",
      "        cbsd       0.47      0.67      0.55        24\n",
      "         cgm       0.40      0.11      0.17        19\n",
      "         cmd       0.42      0.73      0.54        26\n",
      "     healthy       0.55      0.26      0.35        23\n",
      "\n",
      "    accuracy                           0.44       106\n",
      "   macro avg       0.44      0.41      0.39       106\n",
      "weighted avg       0.45      0.44      0.40       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "pred = model.predict(xtest)\n",
    "y_true = np.argmax(ytest, axis=-1)\n",
    "print(\"Validation Labels\")\n",
    "ass=np.random.randint(0, high=xtest.shape[0], size=5, dtype=int)\n",
    "print(y_true[ass])\n",
    "a = model.predict(xtest)\n",
    "y_pred = np.argmax(a, axis=-1)\n",
    "print(\"Validation Prediction\")\n",
    "print(y_pred[ass])\n",
    "print('Confusion Matrix')\n",
    "target_names = ['cbb', 'cbsd','cgm','cmd','healthy'] \n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Classification Report')\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.savefig('resnet_simple.png',dpi=95,quality=95)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(cm,labels,title='Confusion matrix',cmap=None,normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\"])\n",
    "plt.show()\n",
    "plt.savefig(model.name+'_accuracy.png')\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"loss\",\"Validation Loss\"])\n",
    "plt.show()\n",
    "plt.savefig(model.name+'_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
